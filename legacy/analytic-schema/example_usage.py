# example_usage.py

# Import necessary components from the analytic schema module.
# - parse_input: Parses CLI-style input strings into structured data.
# - validate_input: Validates the parsed input against a schema to ensure correctness.
# - OutputDoc: Represents a structured document that captures analysis results, messages, and metadata.
from analytic_schema import parse_input, validate_input, OutputDoc

import time  # Used for timing the analysis duration

# ------------------------------
# STEP 1: Parse and Validate Input Parameters
# ------------------------------

# Simulate command-line interface (CLI) parameters as a string. In a real-world case,
# this might come from sys.argv or another input mechanism.
cli_params = (
    "--input-schema-version 1.0.0 "       # Specify the version of the expected input schema
    "--start-dtg 2025-06-01T00:00:00Z "   # Start of the analysis window (datetime in UTC)
    "--end-dtg 2025-06-02T00:00:00Z "     # End of the analysis window (datetime in UTC)
    "--data-source-type file "           # Type of data source (e.g., file, API, stream)
    "--data-source /tmp/conn.csv"        # Location or identifier of the data source
)

# Parse the CLI-like string into a structured dictionary or similar object.
raw = parse_input(cli_params)  # Can also be used directly with a string

# Validate the parsed input against a defined schema to ensure all required fields
# and formats are correct. Returns a cleaned and typed object.
params = validate_input(raw)

# ------------------------------
# STEP 2: Initialize Output Document
# ------------------------------

# Create an instance of OutputDoc to hold the analysis results.
# - `input_data_hash` is a unique fingerprint of the input data, used for traceability.
#   Here, it's a dummy SHA-256 hash used as a placeholder.
# - `inputs` stores the validated input parameters.
out = OutputDoc(
    input_data_hash="f2ca1bb6c7e907d06dafe4687e579fce76b37e4e93b7605022da52e6ccc26fd2",  # Example hash
    inputs=params
)

# Add informational log messages to the document for traceability and debugging.
out.add_message("INFO", "Parameters processed successfully")

# ------------------------------
# STEP 3: Perform Analysis (Simulated)
# ------------------------------

# Mark the start of the analysis in the output log.
out.add_message("INFO", "Analysis started")

# Start a timer to measure analysis duration.
start = time.perf_counter()

# [Placeholder for actual analysis logic.]
# This could involve scanning files, running analytics on data, etc.
# For demonstration, we'll simulate results directly below.

# Simulated output:
total = 123  # Total number of records processed (example value)

# List of findings generated by the analysis.
# Each finding includes detailed metadata and context.
findings = [
    {
        "finding_id": "123e4567-e89b-12d3-a456-426614174000",  # Unique identifier (UUID)
        "title": "Suspicious DNS query",                       # Short summary of the finding
        "description": "High‚Äêvolume NXDOMAIN ...",             # Full description of what was found
        "event_dtg": "2025-06-07T12:34:56Z",                   # When the event was observed
        "severity": "high",                                    # Severity level (e.g., low, medium, high)
        "confidence": "0.85",                                  # Confidence score (0 to 1)
        "observables": ["evil.example.com"],                   # Entities observed (e.g., domains, IPs)
        "mitre_attack_tactics": ["TA0001"],                    # MITRE ATT&CK tactics involved
        "mitre_attack_techniques": ["T1001"],                  # MITRE ATT&CK techniques
        "recommended_actions": "Block domain",                 # Suggested remediation
        "recommended_pivots": "Check DNS logs",                # Suggested follow-up investigation
        "classification": "U"                                  # Classification level (e.g., Unclassified)
    }
]

# Calculate how long the analysis took in milliseconds.
duration = (time.perf_counter() - start) * 1000

# Log the completion of analysis and results.
out.add_message("INFO", "Analysis complete in %d seconds" % duration)
out.add_message("INFO", "Found %d records" % total)

# Add structured output fields to the document.
# - `records_processed`: Total number of records analyzed
# - `findings`: List of findings from the analysis
out["records_processed"] = total
out["findings"] = findings

# Finalize the document. This validates internal consistency,
# and performs final formatting.
out.finalise()

# Save the structured output to a JSON file for further use or audit.
out.save("notebook_output.json")
