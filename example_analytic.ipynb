{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfb9cf8",
   "metadata": {},
   "source": [
    "# Analytic Contract Schema - Comprehensive Example\n",
    "\n",
    "This notebook provides a **detailed, step-by-step demonstration** of the `contract_schema` library using the bundled `analytic_schema.json` contract.\n",
    "\n",
    "## What This Notebook Demonstrates\n",
    "\n",
    "1. **Contract Loading**: How to load and access contract metadata\n",
    "2. **Input Parsing & Validation**: Multiple ways to parse inputs (dict, CLI args)\n",
    "3. **Failure Modes**: Validation errors when required inputs are missing\n",
    "4. **Field Mapping (data_map)**: Transforming non-SchemaONE data to compliant format\n",
    "5. **Document Creation**: Setting all required and optional output fields\n",
    "6. **Message Logging**: Using `add_message()` for structured execution logging\n",
    "7. **Finding Generation**: Creating findings with all required fields\n",
    "8. **Finalisation**: Auto-computed fields (hashes, timestamps, environment)\n",
    "9. **Export**: Saving documents as JSON and Markdown reports\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15fdfc",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e05773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# contract_schema imports\n",
    "from contract_schema import Contract, SchemaError, to_markdown_card\n",
    "from contract_schema import utils\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=\"INFO\",\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    ")\n",
    "log = logging.getLogger(\"contract_schema.examples\")\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e203cd",
   "metadata": {},
   "source": [
    "## 2. Contract Loading\n",
    "\n",
    "The `Contract` class is the main entry point for working with schema contracts. Loading a contract:\n",
    "- Validates the schema against the meta-schema\n",
    "- Extracts metadata (title, description, version)\n",
    "- Provides access to input and output schemas\n",
    "\n",
    "### Available Bundled Contracts\n",
    "- `analytic_schema.json` - For security analytics and data analysis pipelines\n",
    "- `model_schema.json` - For ML model training manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85698f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the analytic contract from the bundled schemas\n",
    "contract = Contract.load(\"analytic_schema.json\")\n",
    "\n",
    "# Access contract metadata\n",
    "print(\"=\" * 60)\n",
    "print(\"CONTRACT METADATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Title:       {contract.title}\")\n",
    "print(f\"Version:     {contract.version}\")\n",
    "print(f\"Description: {contract.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09013c",
   "metadata": {},
   "source": [
    "### 2.1 Exploring the Input Schema\n",
    "\n",
    "The input schema defines all the parameters that can be passed to an analytic. Let's examine each field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5722cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INPUT SCHEMA FIELDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "input_fields = contract.input_schema.get(\"fields\", {})\n",
    "for field_name, field_spec in input_fields.items():\n",
    "    required = \"REQUIRED\" if field_spec.get(\"required\", False) else \"optional\"\n",
    "    field_type = field_spec.get(\"type\", [\"any\"])\n",
    "    default = field_spec.get(\"default\", \"N/A\")\n",
    "    enum_vals = field_spec.get(\"enum\", None)\n",
    "    desc = field_spec.get(\"description\", \"No description\")\n",
    "    \n",
    "    print(f\"\\n{field_name} ({required})\")\n",
    "    print(f\"  Type: {field_type}\")\n",
    "    if default != \"N/A\":\n",
    "        print(f\"  Default: {default}\")\n",
    "    if enum_vals:\n",
    "        print(f\"  Enum: {enum_vals}\")\n",
    "    print(f\"  Description: {desc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5edc01",
   "metadata": {},
   "source": [
    "### 2.2 Exploring the Output Schema\n",
    "\n",
    "The output schema defines the structure of the result document. Let's see all the fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"OUTPUT SCHEMA FIELDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output_fields = contract.output_schema.get(\"fields\", {})\n",
    "required_count = sum(1 for f in output_fields.values() if f.get(\"required\", False))\n",
    "optional_count = len(output_fields) - required_count\n",
    "\n",
    "print(f\"Total fields: {len(output_fields)}\")\n",
    "print(f\"Required: {required_count}, Optional: {optional_count}\")\n",
    "print()\n",
    "\n",
    "# List required fields\n",
    "print(\"REQUIRED OUTPUT FIELDS:\")\n",
    "for field_name, field_spec in output_fields.items():\n",
    "    if field_spec.get(\"required\", False):\n",
    "        print(f\"  - {field_name}\")\n",
    "\n",
    "print()\n",
    "print(\"OPTIONAL OUTPUT FIELDS:\")\n",
    "for field_name, field_spec in output_fields.items():\n",
    "    if not field_spec.get(\"required\", False):\n",
    "        print(f\"  - {field_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe1c1a",
   "metadata": {},
   "source": [
    "## 3. Failure Modes - Validation Errors\n",
    "\n",
    "This section demonstrates how the module properly throws errors when validation fails. Understanding these error cases is crucial for building robust analytics.\n",
    "\n",
    "### 3.1 Missing Required Input Fields\n",
    "\n",
    "The analytic schema requires the following input fields:\n",
    "- `start_dtg` (required)\n",
    "- `end_dtg` (required)\n",
    "- `data_source_type` (required)\n",
    "- `data_source` (required)\n",
    "\n",
    "Let's see what happens when we try to validate without these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e741763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILURE MODE 1: Missing all required fields\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 1: Missing all required fields\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Attempt to validate an empty input\n",
    "    inputs = contract.parse_and_validate_input({})\n",
    "    print(\"ERROR: This should not print - validation should fail!\")\n",
    "except SchemaError as e:\n",
    "    print(f\"SchemaError raised as expected!\")\n",
    "    print(f\"  Error message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc55133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILURE MODE 2: Missing some required fields (partial input)\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 2: Missing some required fields\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Only provide start_dtg, missing end_dtg, data_source_type, data_source\n",
    "    inputs = contract.parse_and_validate_input({\n",
    "        \"start_dtg\": utils._now_iso(),\n",
    "    })\n",
    "    print(\"ERROR: This should not print - validation should fail!\")\n",
    "except SchemaError as e:\n",
    "    print(f\"SchemaError raised as expected!\")\n",
    "    print(f\"  Error message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILURE MODE 3: Invalid enum value for data_source_type\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 3: Invalid enum value\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "now_iso = utils._now_iso()\n",
    "\n",
    "try:\n",
    "    # data_source_type must be one of: \"file\", \"IONIC\", \"api\"\n",
    "    inputs = contract.parse_and_validate_input({\n",
    "        \"start_dtg\": now_iso,\n",
    "        \"end_dtg\": now_iso,\n",
    "        \"data_source_type\": \"invalid_type\",  # This is not a valid enum value!\n",
    "        \"data_source\": \"/path/to/data.csv\",\n",
    "    })\n",
    "    print(\"ERROR: This should not print - validation should fail!\")\n",
    "except SchemaError as e:\n",
    "    print(f\"SchemaError raised as expected!\")\n",
    "    print(f\"  Error message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILURE MODE 4: Invalid verbosity level\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 4: Invalid verbosity enum value\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # verbosity must be one of: \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"FATAL\"\n",
    "    inputs = contract.parse_and_validate_input({\n",
    "        \"start_dtg\": now_iso,\n",
    "        \"end_dtg\": now_iso,\n",
    "        \"data_source_type\": \"file\",\n",
    "        \"data_source\": \"/path/to/data.csv\",\n",
    "        \"verbosity\": \"VERBOSE\",  # This is not valid!\n",
    "    })\n",
    "    print(\"ERROR: This should not print - validation should fail!\")\n",
    "except SchemaError as e:\n",
    "    print(f\"SchemaError raised as expected!\")\n",
    "    print(f\"  Error message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILURE MODE 5: Invalid date-time format\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE 5: Invalid date-time format\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # start_dtg and end_dtg must be valid ISO 8601 date-time strings\n",
    "    inputs = contract.parse_and_validate_input({\n",
    "        \"start_dtg\": \"not-a-valid-datetime\",  # Invalid format!\n",
    "        \"end_dtg\": now_iso,\n",
    "        \"data_source_type\": \"file\",\n",
    "        \"data_source\": \"/path/to/data.csv\",\n",
    "    })\n",
    "    print(\"ERROR: This should not print - validation should fail!\")\n",
    "except SchemaError as e:\n",
    "    print(f\"SchemaError raised as expected!\")\n",
    "    print(f\"  Error message: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb2fd6",
   "metadata": {},
   "source": [
    "## 4. Successful Input Validation\n",
    "\n",
    "Now let's see how to properly construct and validate inputs with ALL fields (required + optional).\n",
    "\n",
    "### 4.1 Input Fields Reference\n",
    "\n",
    "| Field | Required | Type | Description |\n",
    "|-------|----------|------|-------------|\n",
    "| `start_dtg` | Yes | string (date-time) | Inclusive UTC timestamp marking data window start |\n",
    "| `end_dtg` | Yes | string (date-time) | Exclusive UTC timestamp marking data window end |\n",
    "| `data_source_type` | Yes | string (enum) | Transport mechanism: \"file\", \"IONIC\", \"api\" |\n",
    "| `data_source` | Yes | string | Path, identifier, or URL for data |\n",
    "| `log_path` | No | string | Where to write logs (default: \"stdout\") |\n",
    "| `output` | No | string | Destination for findings (default: \"stdout\") |\n",
    "| `analytic_parameters` | No | object/string | Analytic-specific tuning knobs (default: {}) |\n",
    "| `data_map` | No | object/string | Field mapping for non-SchemaONE data (default: {}) |\n",
    "| `verbosity` | No | string (enum) | Log level: DEBUG/INFO/WARN/ERROR/FATAL (default: \"INFO\") |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d5f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare timestamps\n",
    "now_iso = utils._now_iso()\n",
    "\n",
    "# Define the complete input with ALL fields\n",
    "complete_inputs = {\n",
    "    # --- Required fields ---\n",
    "    \"start_dtg\": now_iso,          # Inclusive UTC timestamp for data window start\n",
    "    \"end_dtg\": now_iso,            # Exclusive UTC timestamp for data window end\n",
    "    \"data_source_type\": \"file\",    # Transport mechanism: \"file\", \"IONIC\", or \"api\"\n",
    "    \"data_source\": \"/data/iris_dataset.csv\",  # Path to the data file\n",
    "\n",
    "    # --- Optional fields (explicitly set for demonstration) ---\n",
    "    \"log_path\": \"stdout\",          # Default: \"stdout\"\n",
    "    \"output\": \"./output/results\",  # Where to write findings\n",
    "    \"analytic_parameters\": {       # Custom tuning parameters\n",
    "        \"min_samples\": 10,\n",
    "        \"threshold\": 0.95,\n",
    "        \"include_summary\": True,\n",
    "    },\n",
    "    \"data_map\": {},                # Empty for now, will demo later\n",
    "    \"verbosity\": \"INFO\",           # Log level\n",
    "}\n",
    "\n",
    "# Parse and validate\n",
    "inputs = contract.parse_and_validate_input(complete_inputs)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INPUT VALIDATION SUCCESSFUL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Validated {len(inputs)} fields:\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd2939",
   "metadata": {},
   "source": [
    "## 5. Field Mapping (data_map) Feature\n",
    "\n",
    "The `data_map` field enables analytics to work with non-SchemaONE data sources by defining mappings from vendor-specific field names to standardized SchemaONE equivalents.\n",
    "\n",
    "### Why Field Mapping?\n",
    "- Ingest data from legacy systems with proprietary field names\n",
    "- Work with third-party APIs that use different naming conventions\n",
    "- Normalize data from multiple vendors into a common schema\n",
    "\n",
    "### SchemaONE Common Fields\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `src_ip` | Source IP address |\n",
    "| `dst_ip` | Destination IP address |\n",
    "| `src_port` | Source port number |\n",
    "| `dst_port` | Destination port number |\n",
    "| `timestamp` | Event timestamp (ISO 8601) |\n",
    "| `protocol` | Network protocol |\n",
    "| `bytes_in` | Bytes received |\n",
    "| `bytes_out` | Bytes sent |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a Zeek conn.log data source with vendor-specific field names\n",
    "zeek_sample_data = pd.DataFrame({\n",
    "    # Zeek uses different field names than SchemaONE\n",
    "    \"ts\": [\"2025-01-15T10:30:00Z\", \"2025-01-15T10:30:01Z\"],  # Zeek timestamp\n",
    "    \"id.orig_h\": [\"192.168.1.100\", \"10.0.0.50\"],             # Zeek source IP\n",
    "    \"id.resp_h\": [\"8.8.8.8\", \"1.1.1.1\"],                     # Zeek dest IP\n",
    "    \"id.orig_p\": [54321, 12345],                             # Zeek source port\n",
    "    \"id.resp_p\": [443, 80],                                  # Zeek dest port\n",
    "    \"proto\": [\"tcp\", \"tcp\"],                                 # Protocol\n",
    "    \"service\": [\"ssl\", \"http\"],                              # Service type\n",
    "    \"orig_bytes\": [1500, 2048],                              # Bytes out\n",
    "    \"resp_bytes\": [3200, 4096],                              # Bytes in\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL ZEEK DATA (Vendor-Specific Field Names)\")\n",
    "print(\"=\" * 60)\n",
    "print(zeek_sample_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the field mapping from Zeek format to SchemaONE\n",
    "zeek_to_schemaone_map = {\n",
    "    \"ts\": \"timestamp\",\n",
    "    \"id.orig_h\": \"src_ip\",\n",
    "    \"id.resp_h\": \"dst_ip\",\n",
    "    \"id.orig_p\": \"src_port\",\n",
    "    \"id.resp_p\": \"dst_port\",\n",
    "    \"proto\": \"protocol\",\n",
    "    \"service\": \"service\",\n",
    "    \"orig_bytes\": \"bytes_out\",\n",
    "    \"resp_bytes\": \"bytes_in\",\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FIELD MAPPING: Zeek -> SchemaONE\")\n",
    "print(\"=\" * 60)\n",
    "for zeek_field, schema_field in zeek_to_schemaone_map.items():\n",
    "    print(f\"  {zeek_field:15} -> {schema_field}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_field_mapping(data: pd.DataFrame, field_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a field mapping to transform vendor-specific column names to SchemaONE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Input DataFrame with vendor-specific column names\n",
    "    field_map : dict\n",
    "        Mapping from vendor field names to SchemaONE field names\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns renamed to SchemaONE equivalents\n",
    "    \"\"\"\n",
    "    rename_map = {\n",
    "        old_name: new_name\n",
    "        for old_name, new_name in field_map.items()\n",
    "        if old_name in data.columns\n",
    "    }\n",
    "    return data.rename(columns=rename_map)\n",
    "\n",
    "\n",
    "# Apply the mapping\n",
    "schemaone_data = apply_field_mapping(zeek_sample_data, zeek_to_schemaone_map)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMED DATA (SchemaONE Field Names)\")\n",
    "print(\"=\" * 60)\n",
    "print(schemaone_data.to_string(index=False))\n",
    "\n",
    "print()\n",
    "print(\"Column transformation successful!\")\n",
    "print(f\"  Original columns: {list(zeek_sample_data.columns)}\")\n",
    "print(f\"  SchemaONE columns: {list(schemaone_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's validate inputs WITH the data_map field\n",
    "inputs_with_mapping = contract.parse_and_validate_input({\n",
    "    \"start_dtg\": now_iso,\n",
    "    \"end_dtg\": now_iso,\n",
    "    \"data_source_type\": \"file\",\n",
    "    \"data_source\": \"zeek_conn.log\",\n",
    "    \"data_map\": zeek_to_schemaone_map,  # Store the mapping for audit purposes\n",
    "    \"verbosity\": \"INFO\",\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INPUT VALIDATION WITH data_map\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"data_map stored: {len(inputs_with_mapping['data_map'])} field mappings\")\n",
    "print(json.dumps(inputs_with_mapping['data_map'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd85a71",
   "metadata": {},
   "source": [
    "## 6. Document Creation\n",
    "\n",
    "The `Document` class is a dict subclass that:\n",
    "- Tracks the output schema for validation\n",
    "- Auto-populates `initialization_dtg` on creation\n",
    "- Provides `add_message()` for structured logging\n",
    "- Computes hashes and environment info on `finalise()`\n",
    "\n",
    "### Output Fields Reference\n",
    "\n",
    "The analytic output schema has many required and optional fields. Let's create a document that demonstrates ALL of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset for demonstration\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET: Iris (for demonstration)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Target classes: {list(iris.target_names)}\")\n",
    "print()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d3359",
   "metadata": {},
   "source": [
    "### 6.1 Generating Findings\n",
    "\n",
    "Each finding represents a detection or observation from the analytic. ALL fields are required:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `finding_id` | string | Unique identifier (UUID v4 recommended) |\n",
    "| `title` | string | Concise summary of detection |\n",
    "| `description` | string | Detailed explanation |\n",
    "| `event_dtg` | string (date-time) | UTC timestamp of the event |\n",
    "| `severity` | string | Impact level: low/medium/high/critical |\n",
    "| `confidence` | string | Probability the finding is valid |\n",
    "| `observables` | list[string] | Artifacts (IPs, hashes, usernames) |\n",
    "| `mitre_attack_tactics` | list[string] | MITRE ATT&CK tactics |\n",
    "| `mitre_attack_techniques` | list[string] | MITRE ATT&CK techniques |\n",
    "| `recommended_actions` | string | Response guidance |\n",
    "| `recommended_pivots` | string | Suggested data sources for context |\n",
    "| `classification` | string | Data handling classification (U, CUI, etc.) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the dataset\n",
    "class_counts = df[\"target\"].value_counts().to_dict()\n",
    "feature_stats = df.describe()\n",
    "\n",
    "# Generate multiple findings to demonstrate the structure\n",
    "findings = [\n",
    "    {\n",
    "        # Finding 1: Class distribution analysis\n",
    "        \"finding_id\": str(uuid.uuid4()),\n",
    "        \"title\": \"Class distribution analysis\",\n",
    "        \"description\": (\n",
    "            f\"Dataset contains {len(df)} samples across {len(class_counts)} classes. \"\n",
    "            f\"Class distribution: setosa={class_counts.get(0, 0)}, \"\n",
    "            f\"versicolor={class_counts.get(1, 0)}, \"\n",
    "            f\"virginica={class_counts.get(2, 0)}. \"\n",
    "            \"Classes are balanced with 50 samples each.\"\n",
    "        ),\n",
    "        \"event_dtg\": utils._now_iso(),\n",
    "        \"severity\": \"low\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"observables\": [\"iris-setosa\", \"iris-versicolor\", \"iris-virginica\"],\n",
    "        \"mitre_attack_tactics\": [],  # Non-security analytic\n",
    "        \"mitre_attack_techniques\": [],\n",
    "        \"recommended_actions\": \"None - informational finding only.\",\n",
    "        \"recommended_pivots\": \"Review feature correlation analysis.\",\n",
    "        \"classification\": \"U\",  # Unclassified\n",
    "    },\n",
    "    {\n",
    "        # Finding 2: Feature range summary\n",
    "        \"finding_id\": str(uuid.uuid4()),\n",
    "        \"title\": \"Feature value ranges\",\n",
    "        \"description\": (\n",
    "            f\"Feature ranges: sepal_length [{df['sepal length (cm)'].min():.1f}, \"\n",
    "            f\"{df['sepal length (cm)'].max():.1f}], \"\n",
    "            f\"sepal_width [{df['sepal width (cm)'].min():.1f}, \"\n",
    "            f\"{df['sepal width (cm)'].max():.1f}], \"\n",
    "            f\"petal_length [{df['petal length (cm)'].min():.1f}, \"\n",
    "            f\"{df['petal length (cm)'].max():.1f}], \"\n",
    "            f\"petal_width [{df['petal width (cm)'].min():.1f}, \"\n",
    "            f\"{df['petal width (cm)'].max():.1f}]\"\n",
    "        ),\n",
    "        \"event_dtg\": utils._now_iso(),\n",
    "        \"severity\": \"low\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"observables\": list(iris.feature_names),\n",
    "        \"mitre_attack_tactics\": [],\n",
    "        \"mitre_attack_techniques\": [],\n",
    "        \"recommended_actions\": \"Consider normalizing features before ML training.\",\n",
    "        \"recommended_pivots\": \"N/A\",\n",
    "        \"classification\": \"U\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATED FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total findings: {len(findings)}\")\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"\\n{i}. {finding['title']}\")\n",
    "    print(f\"   ID: {finding['finding_id']}\")\n",
    "    print(f\"   Severity: {finding['severity']}, Confidence: {finding['confidence']}\")\n",
    "    print(f\"   Observables: {finding['observables'][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8b8f1",
   "metadata": {},
   "source": [
    "### 6.2 Creating the Document with ALL Fields\n",
    "\n",
    "Now we create the output document with all required and optional fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validated inputs from earlier\n",
    "inputs = inputs_with_mapping\n",
    "\n",
    "# Create the document with ALL fields\n",
    "doc = contract.create_document(\n",
    "    # =========================================================================\n",
    "    # PROVENANCE & AUTHORSHIP (Required)\n",
    "    # =========================================================================\n",
    "    input_schema_version=\"1.0.1\",\n",
    "    output_schema_version=contract.version,\n",
    "    author=\"Notebook Author\",\n",
    "    author_organization=\"Example Organization\",\n",
    "    contact=\"author@example.com\",\n",
    "    license=\"Apache-2.0\",\n",
    "    documentation_link=\"https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset\",\n",
    "\n",
    "    # =========================================================================\n",
    "    # CONTRIBUTORS (Optional)\n",
    "    # =========================================================================\n",
    "    contributors={\n",
    "        \"Alice Smith\": \"Data preprocessing and validation\",\n",
    "        \"Bob Jones\": \"Code review and testing\",\n",
    "        \"Charlie Brown\": \"Documentation\",\n",
    "    },\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXECUTION STATUS (Required)\n",
    "    # =========================================================================\n",
    "    status=\"success\",\n",
    "    exit_code=0,\n",
    "\n",
    "    # =========================================================================\n",
    "    # DATASET METADATA (Required)\n",
    "    # =========================================================================\n",
    "    dataset_description=(\n",
    "        \"Fisher's Iris flower data set (1936) containing 150 samples of iris \"\n",
    "        \"flowers with 4 features each. Features include sepal length/width and \"\n",
    "        \"petal length/width measurements.\"\n",
    "    ),\n",
    "    dataset_size=len(df),\n",
    "    dataset_hash=utils._hash(df),\n",
    "    data_schema={\n",
    "        **{c: \"number\" for c in iris.feature_names},\n",
    "        \"target\": \"integer\"\n",
    "    },\n",
    "    feature_names=list(iris.feature_names),\n",
    "\n",
    "    # =========================================================================\n",
    "    # ANALYTIC METADATA (Required)\n",
    "    # =========================================================================\n",
    "    inputs=inputs,\n",
    "    analytic_id=\"example_analytic.ipynb\",\n",
    "    analytic_name=\"Iris Dataset Analyzer\",\n",
    "    analytic_version=\"1.0.0\",\n",
    "    analytic_description=(\n",
    "        \"Demonstration analytic that analyzes the Iris dataset distribution \"\n",
    "        \"and feature characteristics. Generates findings about class balance \"\n",
    "        \"and feature value ranges.\"\n",
    "    ),\n",
    "\n",
    "    # =========================================================================\n",
    "    # FINDINGS (Required)\n",
    "    # =========================================================================\n",
    "    findings=findings,\n",
    "\n",
    "    # =========================================================================\n",
    "    # ADDITIONAL RUN PROPERTIES (Optional)\n",
    "    # =========================================================================\n",
    "    additional_run_properties={\n",
    "        \"class_counts\": class_counts,\n",
    "        \"notebook_environment\": \"Jupyter\",\n",
    "        \"field_mapping_applied\": True,\n",
    "        \"source_format\": \"Zeek conn.log\",\n",
    "        \"target_format\": \"SchemaONE\",\n",
    "        \"ci_job_url\": \"https://example.com/ci/job/12345\",\n",
    "        \"git_commit\": \"abc123def456\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DOCUMENT CREATED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Document has {len(doc)} fields set\")\n",
    "print(f\"initialization_dtg: {doc.get('initialization_dtg', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc7fd1",
   "metadata": {},
   "source": [
    "## 7. Message Logging with `add_message()`\n",
    "\n",
    "The `add_message()` method appends timestamped, leveled log entries to the document's `messages` field. This provides a structured audit trail separate from console logging.\n",
    "\n",
    "### Supported Levels\n",
    "- `DEBUG` - Detailed diagnostic information\n",
    "- `INFO` - General informational messages\n",
    "- `WARN` - Warning messages\n",
    "- `ERROR` - Error messages (non-fatal)\n",
    "- `FATAL` - Fatal error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add structured messages at different levels\n",
    "doc.add_message(\"INFO\", \"Analytic execution started\")\n",
    "doc.add_message(\"DEBUG\", f\"Loaded {len(df)} records from iris dataset\")\n",
    "doc.add_message(\"DEBUG\", f\"Dataset hash: {utils._hash(df)[:16]}...\")\n",
    "doc.add_message(\"INFO\", f\"Applied field mapping: {len(zeek_to_schemaone_map)} fields\")\n",
    "doc.add_message(\"INFO\", f\"Analyzing {len(class_counts)} distinct classes\")\n",
    "doc.add_message(\"DEBUG\", f\"Class distribution: {class_counts}\")\n",
    "doc.add_message(\"INFO\", f\"Generated {len(findings)} finding(s)\")\n",
    "doc.add_message(\"WARN\", \"No anomalies detected in this dataset\")\n",
    "doc.add_message(\"INFO\", \"Analytic execution completed successfully\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MESSAGES ADDED TO DOCUMENT\")\n",
    "print(\"=\" * 60)\n",
    "for i, msg in enumerate(doc.get(\"messages\", []), 1):\n",
    "    print(f\"{i}. [{msg['level']:5}] {msg['timestamp']} - {msg['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b3b36",
   "metadata": {},
   "source": [
    "### 7.1 Message Logging After Finalisation (Failure Mode)\n",
    "\n",
    "Once a document is finalised, `add_message()` becomes a no-op. Let's demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c13d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current message count\n",
    "message_count_before = len(doc.get(\"messages\", []))\n",
    "print(f\"Message count before finalise: {message_count_before}\")\n",
    "print(\"(We will check this again after finalise to show add_message is a no-op)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cee2f",
   "metadata": {},
   "source": [
    "## 8. Finalisation and Auto-Computed Fields\n",
    "\n",
    "The `finalise()` method performs several important operations:\n",
    "\n",
    "1. Records `finalization_dtg` (current UTC timestamp)\n",
    "2. Computes `total_runtime_seconds` from init to finalization\n",
    "3. Generates a unique `run_id` (UUID v4)\n",
    "4. Computes `input_hash` from the inputs dict\n",
    "5. Computes `findings_hash` from the findings list\n",
    "6. Captures `execution_environment` (Python version, libraries, OS, hardware)\n",
    "7. Validates the complete document against the output schema\n",
    "8. Marks the document as immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalise the document\n",
    "doc.finalise()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DOCUMENT FINALISED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nAuto-computed fields:\")\n",
    "print(f\"  run_id:                {doc['run_id']}\")\n",
    "print(f\"  initialization_dtg:    {doc['initialization_dtg']}\")\n",
    "print(f\"  finalization_dtg:      {doc['finalization_dtg']}\")\n",
    "print(f\"  total_runtime_seconds: {doc['total_runtime_seconds']}\")\n",
    "print(f\"  input_hash:            {doc['input_hash'][:32]}...\")\n",
    "print(f\"  findings_hash:         {doc['findings_hash'][:32]}...\")\n",
    "\n",
    "print(\"\\nExecution Environment:\")\n",
    "env = doc[\"execution_environment\"]\n",
    "print(f\"  Python version: {env['python_version']}\")\n",
    "print(f\"  OS: {env['operating_system']}\")\n",
    "print(f\"  User: {env['username']}\")\n",
    "print(f\"  Hardware: CPU={env['hardware_specs']['cpu']}, RAM={env['hardware_specs']['ram']}\")\n",
    "print(f\"  Libraries: {env['library_dependencies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that add_message is a no-op after finalise\n",
    "doc.add_message(\"INFO\", \"This message will NOT be added (document is immutable)\")\n",
    "message_count_after = len(doc.get(\"messages\", []))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFYING IMMUTABILITY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Messages before finalise: {message_count_before}\")\n",
    "print(f\"Messages after finalise:  {message_count_after}\")\n",
    "print(f\"Message was added: {message_count_after > message_count_before}\")\n",
    "print(\"Document is immutable after finalise()!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd0ec0",
   "metadata": {},
   "source": [
    "### 8.1 Finalisation Failure Mode - Missing Required Fields\n",
    "\n",
    "If you try to finalise a document without all required fields, validation will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab62105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an incomplete document to demonstrate validation failure\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE: Incomplete document finalisation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create a document with missing required fields\n",
    "    incomplete_doc = contract.create_document(\n",
    "        # Only set a few fields, missing many required ones\n",
    "        author=\"Test Author\",\n",
    "        status=\"success\",\n",
    "        exit_code=0,\n",
    "    )\n",
    "    incomplete_doc.finalise()\n",
    "    print(\"ERROR: This should not print!\")\n",
    "except SchemaError as e:\n",
    "    print(f\"SchemaError raised as expected!\")\n",
    "    print(f\"  Error: {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1dd81",
   "metadata": {},
   "source": [
    "### 8.2 Saving Before Finalisation (Failure Mode)\n",
    "\n",
    "The document must be finalised before it can be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document and try to save without finalising\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE MODE: Save without finalise\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    unsaved_doc = contract.create_document(\n",
    "        input_schema_version=\"1.0.1\",\n",
    "        output_schema_version=contract.version,\n",
    "        author=\"Test\",\n",
    "        author_organization=\"Test Org\",\n",
    "        contact=\"test@test.com\",\n",
    "        license=\"MIT\",\n",
    "        documentation_link=\"https://example.com\",\n",
    "        status=\"success\",\n",
    "        exit_code=0,\n",
    "        dataset_description=\"Test\",\n",
    "        dataset_size=100,\n",
    "        dataset_hash=\"0\" * 64,\n",
    "        data_schema={},\n",
    "        feature_names=[],\n",
    "        inputs={},\n",
    "        analytic_id=\"test\",\n",
    "        analytic_name=\"Test\",\n",
    "        analytic_version=\"1.0.0\",\n",
    "        analytic_description=\"Test\",\n",
    "        findings=[],\n",
    "    )\n",
    "    # Try to save without finalising\n",
    "    unsaved_doc.save(\"/tmp/test_output.json\")\n",
    "    print(\"ERROR: This should not print!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"RuntimeError raised as expected!\")\n",
    "    print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c4193",
   "metadata": {},
   "source": [
    "## 9. Exporting Results\n",
    "\n",
    "The finalised document can be exported to:\n",
    "1. **JSON** - For programmatic consumption\n",
    "2. **Markdown** - For human-readable reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af85dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary directory for output files\n",
    "output_dir = tempfile.mkdtemp()\n",
    "json_path = os.path.join(output_dir, \"iris_analytic_report.json\")\n",
    "md_path = os.path.join(output_dir, \"iris_analytic_report.md\")\n",
    "\n",
    "# Save to JSON\n",
    "doc.save(json_path)\n",
    "print(f\"JSON saved to: {json_path}\")\n",
    "\n",
    "# Generate and save Markdown report\n",
    "md_content = to_markdown_card(doc)\n",
    "with open(md_path, \"w\") as f:\n",
    "    f.write(md_content)\n",
    "print(f\"Markdown saved to: {md_path}\")\n",
    "\n",
    "# Show file sizes\n",
    "json_size = os.path.getsize(json_path)\n",
    "md_size = os.path.getsize(md_path)\n",
    "print(f\"\\nFile sizes:\")\n",
    "print(f\"  JSON: {json_size:,} bytes\")\n",
    "print(f\"  Markdown: {md_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45522f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the JSON output\n",
    "print(\"=\" * 60)\n",
    "print(\"JSON OUTPUT PREVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    json_content = json.load(f)\n",
    "\n",
    "# Show the top-level keys\n",
    "print(\"Top-level fields in output:\")\n",
    "for key in json_content.keys():\n",
    "    value = json_content[key]\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}: {{...}} ({len(value)} keys)\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"  {key}: [...] ({len(value)} items)\")\n",
    "    elif isinstance(value, str) and len(value) > 50:\n",
    "        print(f'  {key}: \"{value[:50]}...\"')\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the Markdown output\n",
    "print(\"=\" * 60)\n",
    "print(\"MARKDOWN OUTPUT PREVIEW (first 50 lines)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with open(md_path, \"r\") as f:\n",
    "    md_lines = f.readlines()\n",
    "\n",
    "for line in md_lines[:50]:\n",
    "    print(line, end=\"\")\n",
    "    \n",
    "if len(md_lines) > 50:\n",
    "    print(f\"\\n... ({len(md_lines) - 50} more lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af03e800",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "This notebook demonstrated the complete workflow for using the `contract_schema` library with the analytic contract:\n",
    "\n",
    "### Features Demonstrated\n",
    "\n",
    "| Feature | Status |\n",
    "|---------|--------|\n",
    "| Contract Loading | Done |\n",
    "| Input Schema Exploration | Done |\n",
    "| Output Schema Exploration | Done |\n",
    "| Input Validation (Success) | Done |\n",
    "| Input Validation (Failure - Missing Required) | Done |\n",
    "| Input Validation (Failure - Invalid Enum) | Done |\n",
    "| Input Validation (Failure - Invalid DateTime) | Done |\n",
    "| Field Mapping (data_map) | Done |\n",
    "| Document Creation (All Fields) | Done |\n",
    "| Message Logging (add_message) | Done |\n",
    "| Finding Generation | Done |\n",
    "| Finalisation | Done |\n",
    "| Auto-computed Fields | Done |\n",
    "| Immutability After Finalise | Done |\n",
    "| Save Without Finalise (Failure) | Done |\n",
    "| Export to JSON | Done |\n",
    "| Export to Markdown | Done |\n",
    "\n",
    "### Key Classes and Methods\n",
    "\n",
    "```python\n",
    "from contract_schema import Contract, SchemaError, to_markdown_card\n",
    "from contract_schema import utils\n",
    "\n",
    "# Load a contract\n",
    "contract = Contract.load(\"analytic_schema.json\")\n",
    "\n",
    "# Validate inputs\n",
    "inputs = contract.parse_and_validate_input({...})\n",
    "\n",
    "# Create and populate a document\n",
    "doc = contract.create_document(**fields)\n",
    "\n",
    "# Add structured log messages\n",
    "doc.add_message(\"INFO\", \"message text\")\n",
    "\n",
    "# Finalise and validate\n",
    "doc.finalise()\n",
    "\n",
    "# Export\n",
    "doc.save(\"output.json\")\n",
    "to_markdown_card(doc)  # Returns markdown string\n",
    "```\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "Always wrap input validation and finalisation in try/except blocks to handle `SchemaError`:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    inputs = contract.parse_and_validate_input(user_input)\n",
    "except SchemaError as e:\n",
    "    log.error(f\"Input validation failed: {e}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f37197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "import shutil\n",
    "shutil.rmtree(output_dir)\n",
    "print(\"Temporary files cleaned up\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
